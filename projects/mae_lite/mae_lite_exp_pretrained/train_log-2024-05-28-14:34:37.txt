[Rank #0] | 2024-05-28 at 14:34:37 | INFO | gpuid: 0, args: <>Namespace(dist_backend='nccl', dist_url='tcp://cnvrg-job-notebooksession-ktvxrmtyhntfp8cmh8ya-1-678bb7448t4rvx:38267', batch_size=1024, max_epoch=300, devices='0,1,2,3,4,5,6,7', eval=True, exp_file='mae_lite_exp.py', ckpt='/cnvrg/model/mae_tiny_400e.pth.tar', amp=True, exp_options={'pretrain_exp_name': '/cnvrg/outputs/mae_tiny_400e_pretrained'}, world_size=8)
[Rank #0] | 2024-05-28 at 14:34:37 | INFO | Used experiment configs:
=====================  =======================================
config key             value
=====================  =======================================
batch_size             1024
max_epoch              300
seed                   0
data_format            image
clip_grad              <class 'NoneType'>
clip_mode              norm
output_dir             /cnvrg/outputs
exp_name               /cnvrg/projects/mae_lite/mae_lite_exp
print_interval         10
dump_interval          10
eval_interval          10
enable_tensorboard     True
dataset                Fundus
transform              typical_imagenet_transform
image_size             224
encoder_arch           mae_vit_tiny_patch16
pretrained             False
num_classes            1000
global_pool            <class 'NoneType'>
img_size               224
input_size             <class 'NoneType'>
crop_pct               <class 'NoneType'>
mean                   <class 'NoneType'>
std                    <class 'NoneType'>
interpolation
validation_batch_size  <class 'NoneType'>
validation_dataset     <class 'NoneType'>
opt                    adamw
opt_eps                1e-08
opt_betas              (0.9, 0.95)
momentum               0.9
weight_decay           0.05
sched                  warmcos
basic_lr_per_img       5.859375e-07
lr_noise               <class 'NoneType'>
lr_noise_pct           0.67
lr_noise_std           1.0
lr_cycle_mul           1.0
lr_cycle_decay         0.5
lr_cycle_limit         1
lr_k_decay             1.0
warmup_lr_per_img      7.8125e-07
min_lr_per_img         7.8125e-09
epochs                 300
epoch_repeats          0
start_epoch            <class 'NoneType'>
decay_epochs           <class 'NoneType'>
warmup_epochs          40
cooldown_epochs        10
patience_epochs        10
decay_rate             0.1
no_aug                 False
scale                  (0.08, 1.0)
ratio                  (0.75, 1.3333333333333333)
hflip                  0.5
vflip                  0.0
color_jitter           <class 'NoneType'>
aa                     <class 'NoneType'>
reprob                 0.0
remode                 pixel
recount                1
resplit                False
mixup                  0.0
cutmix                 0.0
cutmix_minmax          <class 'NoneType'>
mixup_prob             1.0
mixup_switch_prob      0.5
mixup_mode             batch
mixup_off_epoch        0
smoothing              0.0
train_interpolation    random
drop                   0.0
drop_connect           <class 'NoneType'>
drop_path              <class 'NoneType'>
drop_block             <class 'NoneType'>
bn_tf                  False
bn_momentum            <class 'NoneType'>
bn_eps                 <class 'NoneType'>
sync_bn                False
dist_bn                reduce
model_ema              False
model_ema_force_cpu    False
model_ema_decay        0.9998
num_workers            10
weights_prefix         model
norm_pix_loss          True
mask_ratio             0.75
warmup_lr              0.0
min_lr                 0.0
save_folder_prefix
pretrain_exp_name      /cnvrg/outputs/mae_tiny_400e_pretrained
=====================  =======================================
[Rank #0] | 2024-05-28 at 14:34:37 | INFO | List of override configs:

'pretrain_exp_name' is set to '/cnvrg/outputs/mae_tiny_400e_pretrained'
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Environment info:
----------------------  --------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.4
Pillow                  9.2.0
PyTorch                 2.1.0a0+29c30b1 @/usr/local/lib/python3.10/dist-packages/torch
PyTorch debug build     False
CUDA available          True
GPU 0,1,2,3,4,5,6,7     NVIDIA A100-SXM4-80GB
CUDA_HOME               /usr/local/cuda
NVCC                    Build cuda_12.2.r12.2/compiler.33053471_0
TORCH_CUDA_ARCH_LIST    5.2 6.0 6.1 7.0 7.5 8.0 8.6 9.0+PTX
torchvision             0.16.0a0 @/usr/local/lib/python3.10/dist-packages/torchvision
torchvision arch flags  sm_52, sm_60, sm_61, sm_70, sm_75, sm_80, sm_86, sm_90
cv2                     4.7.0
----------------------  --------------------------------------------------------------
Git status: unknown
----------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 11.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.1-Product Build 20201104 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash N/A)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.2
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_90,code=compute_90
  - CuDNN 8.9.4
  - Magma 2.6.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.2, CUDNN_VERSION=8.9.4, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS=-fno-gnu-unique -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Illustration of model strcutures:
MAE(
  (model): MaskedAutoencoderViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (identity): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
    (decoder_embed): Linear(in_features=192, out_features=96, bias=True)
    (decoder_blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (identity): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=96, out_features=768, bias=True)
  )
)
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Automatic Mixed Precision is enabled!
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Load pretrained checkpoints from /cnvrg/model/mae_tiny_400e.pth.tar.
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Model params [] are not loaded
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | State-dict params [] are not used
[Rank #0] | 2024-05-28 at 14:34:38 | INFO | Training start...
[Rank #0] | 2024-05-28 at 14:34:39 | INFO | ---> start train epoch1
[Rank #0] | 2024-05-28 at 14:34:41 | INFO | [1/300], remain:0d.00h.26m, It:[10/20], Max-Mem:1701M, Data-Time:0.026, LR:0.0000, Loss:0.0221
[Rank #0] | 2024-05-28 at 14:34:42 | INFO | [1/300], remain:0d.00h.17m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0000, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:34:42 | INFO | ---> start train epoch2
[Rank #0] | 2024-05-28 at 14:34:44 | INFO | [2/300], remain:0d.00h.11m, It:[10/20], Max-Mem:1701M, Data-Time:0.024, LR:0.0000, Loss:0.0228
[Rank #0] | 2024-05-28 at 14:34:45 | INFO | [2/300], remain:0d.00h.09m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0000, Loss:0.0227
[Rank #0] | 2024-05-28 at 14:34:45 | INFO | ---> start train epoch3
[Rank #0] | 2024-05-28 at 14:34:47 | INFO | [3/300], remain:0d.00h.12m, It:[10/20], Max-Mem:1701M, Data-Time:0.029, LR:0.0000, Loss:0.0221
[Rank #0] | 2024-05-28 at 14:34:48 | INFO | [3/300], remain:0d.00h.09m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0000, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:34:48 | INFO | ---> start train epoch4
[Rank #0] | 2024-05-28 at 14:34:49 | INFO | [4/300], remain:0d.00h.10m, It:[10/20], Max-Mem:1701M, Data-Time:0.026, LR:0.0001, Loss:0.0221
[Rank #0] | 2024-05-28 at 14:34:50 | INFO | [4/300], remain:0d.00h.09m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0219
[Rank #0] | 2024-05-28 at 14:34:50 | INFO | ---> start train epoch5
[Rank #0] | 2024-05-28 at 14:34:51 | INFO | [5/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.029, LR:0.0001, Loss:0.0217
[Rank #0] | 2024-05-28 at 14:34:52 | INFO | [5/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0224
[Rank #0] | 2024-05-28 at 14:34:52 | INFO | ---> start train epoch6
[Rank #0] | 2024-05-28 at 14:34:54 | INFO | [6/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.030, LR:0.0001, Loss:0.0230
[Rank #0] | 2024-05-28 at 14:34:55 | INFO | [6/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0207
[Rank #0] | 2024-05-28 at 14:34:55 | INFO | ---> start train epoch7
[Rank #0] | 2024-05-28 at 14:34:56 | INFO | [7/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.025, LR:0.0001, Loss:0.0232
[Rank #0] | 2024-05-28 at 14:34:57 | INFO | [7/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:34:57 | INFO | ---> start train epoch8
[Rank #0] | 2024-05-28 at 14:34:59 | INFO | [8/300], remain:0d.00h.10m, It:[10/20], Max-Mem:1701M, Data-Time:0.025, LR:0.0001, Loss:0.0219
[Rank #0] | 2024-05-28 at 14:34:59 | INFO | [8/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0228
[Rank #0] | 2024-05-28 at 14:34:59 | INFO | ---> start train epoch9
[Rank #0] | 2024-05-28 at 14:35:01 | INFO | [9/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.030, LR:0.0001, Loss:0.0225
[Rank #0] | 2024-05-28 at 14:35:02 | INFO | [9/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0230
[Rank #0] | 2024-05-28 at 14:35:02 | INFO | ---> start train epoch10
[Rank #0] | 2024-05-28 at 14:35:03 | INFO | [10/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.029, LR:0.0001, Loss:0.0232
[Rank #0] | 2024-05-28 at 14:35:04 | INFO | [10/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0001, Loss:0.0211
[Rank #0] | 2024-05-28 at 14:35:04 | INFO | ---> start train epoch11
[Rank #0] | 2024-05-28 at 14:35:06 | INFO | [11/300], remain:0d.00h.08m, It:[10/20], Max-Mem:1701M, Data-Time:0.027, LR:0.0002, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:35:07 | INFO | [11/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0201
[Rank #0] | 2024-05-28 at 14:35:07 | INFO | ---> start train epoch12
[Rank #0] | 2024-05-28 at 14:35:08 | INFO | [12/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.029, LR:0.0002, Loss:0.0215
[Rank #0] | 2024-05-28 at 14:35:09 | INFO | [12/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:35:09 | INFO | ---> start train epoch13
[Rank #0] | 2024-05-28 at 14:35:11 | INFO | [13/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.030, LR:0.0002, Loss:0.0227
[Rank #0] | 2024-05-28 at 14:35:11 | INFO | [13/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0245
[Rank #0] | 2024-05-28 at 14:35:11 | INFO | ---> start train epoch14
[Rank #0] | 2024-05-28 at 14:35:13 | INFO | [14/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.030, LR:0.0002, Loss:0.0205
[Rank #0] | 2024-05-28 at 14:35:14 | INFO | [14/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0215
[Rank #0] | 2024-05-28 at 14:35:14 | INFO | ---> start train epoch15
[Rank #0] | 2024-05-28 at 14:35:15 | INFO | [15/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.032, LR:0.0002, Loss:0.0217
[Rank #0] | 2024-05-28 at 14:35:16 | INFO | [15/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0215
[Rank #0] | 2024-05-28 at 14:35:16 | INFO | ---> start train epoch16
[Rank #0] | 2024-05-28 at 14:35:18 | INFO | [16/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.030, LR:0.0002, Loss:0.0207
[Rank #0] | 2024-05-28 at 14:35:18 | INFO | [16/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0002, Loss:0.0214
[Rank #0] | 2024-05-28 at 14:35:18 | INFO | ---> start train epoch17
[Rank #0] | 2024-05-28 at 14:35:20 | INFO | [17/300], remain:0d.00h.08m, It:[10/20], Max-Mem:1701M, Data-Time:0.032, LR:0.0002, Loss:0.0218
[Rank #0] | 2024-05-28 at 14:35:21 | INFO | [17/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0003, Loss:0.0214
[Rank #0] | 2024-05-28 at 14:35:21 | INFO | ---> start train epoch18
[Rank #0] | 2024-05-28 at 14:35:22 | INFO | [18/300], remain:0d.00h.09m, It:[10/20], Max-Mem:1701M, Data-Time:0.027, LR:0.0003, Loss:0.0239
[Rank #0] | 2024-05-28 at 14:35:23 | INFO | [18/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0003, Loss:0.0215
[Rank #0] | 2024-05-28 at 14:35:23 | INFO | ---> start train epoch19
[Rank #0] | 2024-05-28 at 14:35:25 | INFO | [19/300], remain:0d.00h.08m, It:[10/20], Max-Mem:1701M, Data-Time:0.028, LR:0.0003, Loss:0.0237
[Rank #0] | 2024-05-28 at 14:35:26 | INFO | [19/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0003, Loss:0.0220
[Rank #0] | 2024-05-28 at 14:35:26 | INFO | ---> start train epoch20
[Rank #0] | 2024-05-28 at 14:35:27 | INFO | [20/300], remain:0d.00h.08m, It:[10/20], Max-Mem:1701M, Data-Time:0.031, LR:0.0003, Loss:0.0223
[Rank #0] | 2024-05-28 at 14:35:28 | INFO | [20/300], remain:0d.00h.08m, It:[20/20], Max-Mem:1701M, Data-Time:0.000, LR:0.0003, Loss:0.0217
[Rank #0] | 2024-05-28 at 14:35:28 | INFO | ---> start train epoch21
